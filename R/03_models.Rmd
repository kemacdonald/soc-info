---
title: "Soc-Info Statistical Models"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Setup

```{r}
library(magrittr); library(tidyverse); library(here); 
library(rstanarm); library(brms)
source(here("R/soc_info_helpers.R"))
options(mc.cores=parallel::detectCores())
set.seed (3875)
```

Read data. 

```{r}
d_path <- "data/03_merged_data/"
d <- read_csv(here(d_path, "goal_actions_master.csv")) 
```

# Experiment 1 

## Model condition difference in action selection

Since there were three options, I think we need to use a softmax or multinomial regression. No repeated measurements, so we don't model random effects. From [this](https://www.analyticsvidhya.com/blog/2016/02/multinomial-ordinal-logistic-regression/) article:

> The multinomial logistic regression estimates a separate binary logistic regression model for each dummy variables.  The result is M-1 binary logistic regression models.  Each model conveys the effect of predictors on the probability of success in that category, in comparison to the reference category.

> Remember, multinomial logistic regression reports the odds of being in the different outcome categories in reference to some base group.

> Because of the indeterminacy in the regression coe cients, we can interpret the regression coe cients only relative to the reference category. In softmax regression, the regression coeficients can be conceived in terms of the log odds of each outcome relative to the reference outcome:

```{r}
d_model_e1 <- d %>%
  filter(manip_check_score >= 2, str_detect(experiment, "e1")) %>% 
  distinct(id, condition, action_response, action_r_collap, action_trial_time, 
           entropy_change) %>% 
  mutate(condition_fact = factor(condition, 
                                 levels = c("nogoal", "learning", "presentation",
                                            "performance")),
         action_response_fact = factor(action_response, 
                                       levels = c("button", "handle", "both")),
         action_num = ifelse(action_r_collap == "both", 0, 1))
```

```{r action logit model}
t_prior <- student_t(df = 7, location = 0, scale = 2.5)
m_act_logit <- stan_glm(action_num ~ condition_fact, 
                        data = d_model_e1,
                        family = binomial(link = "logit"),
                        prior = t_prior, prior_intercept = t_prior)

summary(m_act_logit)

samples_logit_act_e1 <- m_act_logit %>% 
  as.data.frame() %>% 
  as_tibble() %>% 
  dplyr::rename(no_goal = `(Intercept)`,
                learning_beta = condition_factlearning,
                performance_beta = condition_factperformance,
                presentation_beta = condition_factpresentation) %>% 
  mutate(sample_id = 1:n()) %>% 
  gather(key = effect, value = param_est, -sample_id)
```

```{r action multinomial model, eval = F}
# fit model
m_act_multi <- brm(action_response_fact ~ condition_fact,
             data = d_model_e1,
             family="categorical", 
             prior=c(set_prior ("normal (0, 8)"))) # non-informative prior

summary(m_act_multi)
```

Extract and clean up the posterior samples.

```{r, eval = F}
samples_multi_act_e1 <- posterior_samples(m_act, "b") %>% 
  mutate(sample_id = seq(1, n())) %>% 
  gather(key = param_name, value = estimate, -sample_id) %>% 
  mutate(param_name = str_replace(param_name, "b_mu", ""),
         param_name = str_replace(param_name, "condition_fact", ""),
         estimate_prob = logit_to_prob(estimate)) %>% 
  separate(param_name, into = c("model", "param_name")) 
```

Plot posterior distributions over model params. 

```{r, eval = F}
samples_multi_act_e1 %>% 
  filter(model == "both") %>% 
  ggplot(aes(x = estimate)) + 
  geom_line(stat = "density", size = 1.5) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  facet_grid(param_name~model, scales = "free_x") +
  labs(title = "Posterior distribution over logit model predicting the 'both' action",
       subtitle = "Reference group is the 'no-goal' condition and 'button' action",
       x = "estimate (logit scale)")
```

## RT Model

```{r}
d_rt_e1 <- d_model_e1 %>% 
  rename(rt = action_trial_time) %>% 
  filter(rt <= 120) %>% 
  mutate(log_rt = log(rt))

m_rt_e1 <- stan_lm(
  log_rt ~ condition_fact,
  data = d_rt_e1,
  prior = R2(0.75),
  adapt_delta = 0.99 # changed from 0.8 to 0.99 because of warning about divergent samples
)
```

Extract posterior samples for RT differences.

```{r}
samples_rt_e1 <- m_rt_e1 %>% 
  as.data.frame() %>% 
  as_tibble() %>% 
  dplyr::rename(no_goal = `(Intercept)`,
                learning_beta = condition_factlearning,
                performance_beta = condition_factperformance,
                presentation_beta = condition_factpresentation) %>% 
  select(-sigma, -R2, -`log-fit_ratio`) %>% 
  mutate(sample_id = 1:n()) %>% 
  gather(key = condition, value = param_est, -sample_id) %>% 
  mutate(rt_sec_scale = exp(param_est))
```

Plot posterior distributions over model params. 

```{r}
samples_rt_e1 %>% 
  filter(condition != "no_goal") %>% 
  ggplot(aes(x = param_est, color = condition)) + 
  ggthemes::scale_color_ptol() +
  geom_line(stat = "density", size = 1.5) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(title = "Posterior distribution over linear model predicting RT on action trial",
       subtitle = "Reference group is the 'no-goal' condition",
       x = "rt estimate (log scale)")
```

## Entropy analysis

Model entropy change across conditions.

```{r explore entropy models}
m_entropy_e1_act <- stan_lm(
  entropy_change ~ condition_fact,
  data = d_model_e1,
  prior = R2(0.75)
)

m_entropy_e1_cond <- stan_lm(
  entropy_change ~ condition_fact,
  data = d_model_e1,
  prior = R2(0.75)
)

m_entropy_e1_int <- stan_lm(
  entropy_change ~ condition_fact * action_r_collap,
  data = d_model_e1,
  prior = R2(0.75)
)

m_entropy_e1_add <- stan_lm(
  entropy_change ~ condition_fact + action_r_collap,
  data = d_model_e1,
  prior = R2(0.75)
)

loo_act <- rstanarm::loo(m_entropy_e1_act)
loo_cond <- rstanarm::loo(m_entropy_e1_cond)
loo_int <- rstanarm::loo(m_entropy_e1_int)
loo_add <- rstanarm::loo(m_entropy_e1_add)

compare_models(loo_add, loo_int)
```

Additive model wins. 

```{r e1 entropy additive model}
samples_entropy_e1_add <- m_entropy_e1_add %>% 
  as.data.frame() %>% 
  as_tibble() %>% 
  dplyr::rename(no_goal = `(Intercept)`,
                learning_beta = condition_factlearning,
                performance_beta = condition_factperformance,
                presentation_beta = condition_factpresentation,
                single_action_beta = action_r_collapsingle) %>% 
  select(-sigma, -R2, -`log-fit_ratio`) %>% 
  mutate(sample_id = 1:n()) %>% 
  gather(key = effect, value = param_est, -sample_id) %>% 
  mutate(eff_type = ifelse(str_detect(effect, "int"), "interaction", "main effect")) 
```

```{r, eval = F}
samples_entropy_e1 <- m_entropy_e1_int %>% 
  as.data.frame() %>% 
  as_tibble() %>% 
  dplyr::rename(no_goal = `(Intercept)`,
                learning_beta = condition_factlearning,
                performance_beta = condition_factperformance,
                presentation_beta = condition_factpresentation,
                single_action_beta = action_r_collapsingle,
                int_present_single_act = `condition_factpresentation:action_r_collapsingle`,
                int_learn_single_act = `condition_factlearning:action_r_collapsingle`,
                int_perf_single_act = `condition_factperformance:action_r_collapsingle`) %>% 
  select(-sigma, -R2, -`log-fit_ratio`) %>% 
  mutate(sample_id = 1:n()) %>% 
  gather(key = effect, value = param_est, -sample_id) %>% 
  mutate(eff_type = ifelse(str_detect(effect, "int"), "interaction", "main effect")) 
```

Plot main effects and interaction betas

```{r}
samples_entropy_e1_add %>% 
  filter(effect != "no_goal") %>% 
  ggplot(aes(x = param_est, color = effect)) + 
  ggthemes::scale_color_ptol() +
  labs(title = "Posteriors over model betas", 
       subtitle = "reference group is the no-goal condition") + 
  geom_line(stat = "density", size = 1.5) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  theme(legend.position = "right")
```

# Experiment 2

```{r e2 filter}
d_model_e2 <- d %>%
  filter(manip_check_score >= 2, str_detect(experiment, "e2")) %>% 
  distinct(id, condition, action_response, action_r_collap, action_trial_time, 
           entropy_change, social_condition) %>% 
  mutate(condition_fact = factor(condition, 
                                 levels = c("nogoal", "performance", "presentation",
                                            "learning")),
         action_response_fact = factor(action_response, 
                                       levels = c("button", "handle", "both")),
         action_num = ifelse(action_r_collap == "both", 0, 1))
```

Action as a function of social context and goal manipulation.

```{r action logit model e2}
t_prior <- student_t(df = 7, location = 0, scale = 2.5)

# remove the presentation condition because it does not have no-social context
m_act_logit_e2_int <- d_model_e2 %>% 
  stan_glm(
    formula = action_num ~ condition_fact * social_condition, 
    data = .,
    family = binomial(link = "logit"),
    prior = t_prior, prior_intercept = t_prior
    )

samples_logit_act_e2_int <- m_act_logit_e2_int %>% 
  as.data.frame() %>% 
  as_tibble() %>% 
  dplyr::rename(no_goal = `(Intercept)`,
                learning_beta = condition_factlearning,
                performance_beta = condition_factperformance,
                social_beta =  social_conditionsocial,
                int_learn_social = `condition_factlearning:social_conditionsocial`,
                int_perf_social = `condition_factperformance:social_conditionsocial`
                ) %>% 
  mutate(sample_id = 1:n()) %>% 
  gather(key = effect, value = param_est, -sample_id) %>% 
  mutate(eff_type = ifelse(str_detect(effect, "int"), "interaction", "main effect")) 
```

Test main effects 

```{r action logit e2}
t_prior <- student_t(df = 7, location = 0, scale = 2.5)

# remove the presentation condition because it does not have no-social context
m_act_logit_e2 <- d_model_e2 %>% 
  stan_glm(action_num ~ condition_fact + social_condition, 
                        data = .,
                        family = binomial(link = "logit"),
                        prior = t_prior, prior_intercept = t_prior)

samples_logit_act_e2 <- m_act_logit_e2 %>% 
  as.data.frame() %>% 
  as_tibble() %>% 
  dplyr::rename(no_goal = `(Intercept)`,
                learning_beta = condition_factlearning,
                performance_beta = condition_factperformance,
                presentation_beta = condition_factpresentation
                ) %>% 
  mutate(sample_id = 1:n()) %>% 
  gather(key = effect, value = param_est, -sample_id) 
```

Compare the interaction and additive models.

```{r}
loo_add_e2 <- rstanarm::loo(m_act_logit_e2)
loo_int_e2 <- rstanarm::loo(m_act_logit_e2_int)

compare_models(loo_add_e2, loo_int_e2)
```

The model with the interaction term is not doing much better

### E2: RT on decision trials 

Modeled as a function of condition and social context.

```{r rt model e2}
d_rt_e2 <- d_model_e2 %>% 
  rename(rt = action_trial_time) %>% 
  filter(rt > 0, rt <= 120) %>% 
  mutate(log_rt = log(rt))

# only use condition
m_rt_e2 <- stan_lm(
  log_rt ~ condition_fact,
  data = d_rt_e2,
  prior = R2(0.75),
  adapt_delta = 0.99 # changed from 0.8 to 0.99 because of warning about divergent samples
)

# add social context as additive effect
m_rt_e2_add <- stan_lm(
  log_rt ~ condition_fact + social_condition,
  data = d_rt_e2,
  prior = R2(0.75)
  #adapt_delta = 0.99 # changed from 0.8 to 0.99 because of warning about divergent samples
)

# add interaction term for goal prompt and social context
m_rt_e2_int <- stan_lm(
  log_rt ~ condition_fact * social_condition,
  data = filter(d_rt_e2, condition != "presentation"),
  prior = R2(0.75)
  #adapt_delta = 0.99 # changed from 0.8 to 0.99 because of warning about divergent samples
)
```

Compare models.

```{r}
loo_rt_e2 <- rstanarm::loo(m_rt_e2)
loo_rt_add_e2 <- rstanarm::loo(m_rt_e2_add)
loo_rt_int_e2 <- rstanarm::loo(m_rt_e2_int)

compare_models(loo_rt_e2, loo_rt_add_e2)
```

The model using just goal condition to predict response times does best. So we extract the posterior samples from that model.

```{r extract rt samples e2}
samples_rt_e2 <- m_rt_e2 %>% 
  as.data.frame() %>% 
  as_tibble() %>% 
  dplyr::rename(no_goal = `(Intercept)`,
                learning_beta = condition_factlearning,
                performance_beta = condition_factperformance,
                presentation_beta = condition_factpresentation) %>% 
  select(-sigma, -R2, -`log-fit_ratio`) %>% 
  mutate(sample_id = 1:n()) %>% 
  gather(key = condition, value = param_est, -sample_id) %>% 
  mutate(rt_sec_scale = exp(param_est))
```

### E2: entropy change across conditions.

```{r}
m_entropy_e2 <- stan_lm(
  entropy_change ~ condition_fact + action_r_collap,
  data = d_model_e2,
  prior = R2(0.75)
)

samples_entropy_e2 <- m_entropy_e2 %>% 
  as.data.frame() %>% 
  as_tibble() %>% 
  dplyr::rename(no_goal = `(Intercept)`,
                learning_beta = condition_factlearning,
                performance_beta = condition_factperformance,
                presentation_beta = condition_factpresentation,
                single_action_beta = action_r_collapsingle) %>% 
  select(-sigma, -R2, -`log-fit_ratio`) %>% 
  mutate(sample_id = 1:n()) %>% 
  gather(key = effect, value = param_est, -sample_id) 
```

## Save posterior samples 

```{r save models and samples}
models <- list(action_model_e1 = m_act_logit,
               rt_model_e1 = m_rt_e1,
               ent_model_e1 = m_entropy_e1_add,
               action_model_e2 = m_act_logit_e2,
               action_model_e2_int = m_act_logit_e2_int,
               rt_model_e1 = m_rt_e2,
               ent_model_e2 = m_entropy_e2
               )

posteriors <- list(action_logit_samples_e1 = samples_logit_act_e1,
                   rt_samples_e1 = samples_rt_e1,
                   ent_samples_e1 = samples_entropy_e1_add,
                   action_logit_samples_e2 = samples_logit_act_e2,
                   action_logit_samples_e2_int = samples_logit_act_e2_int,
                   rt_samples_e2 = samples_rt_e2,
                   ent_samples_e2 = samples_entropy_e2
                   )

saveRDS(posteriors, file = here("data/03_merged_data", "soc-info-posterior-samples.rds"))
saveRDS(models, file = here("data/03_merged_data", "soc-info-models.rds"))
```
