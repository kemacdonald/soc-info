---
title: "Learning versus performance in social contexts"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Erica J. Yoon*}, {\large \bf Kyle MacDonald*}, {\large \bf Mika Asaba}, {\large \bf Hyowon Gweon}, \and {\large \bf Michael C. Frank} \\ \{ejyoon, kylem4, masaba, hyo, mcfrank\} @stanford.edu \\ Department of Psychology, Stanford University \\ *These authors contributed equally to this work.}

abstract: 
    "..."
    
keywords:
    "Learning; social context; information gain; OED; self-presentation; goal tradeoff"
    
output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(gridExtra)
library(ggplot2)
library(xtable)
```

# Introduction

Imagine that you are a novice cook and you have to decide what meal to prepare for a first date. Should you choose a recipe that you have tried before or should you attempt to make something new? While the familiar recipe has a high chance of ensuring a good meal, you are less likely to discover a new, delicious dish. The new recipe might taste even better, but it has a higher chance of turning out awful since you have never made it before. Now, consider how your decision making might change if instead of making the meal for first date, you were preparing it for a charitable cooking teacher whom you trust.

Scenarios like this, capture an "explore-exploit” dilemma [@sutton1998], in which we have to choose between actions that could (a) lead to an overt, readily accessible reward based on what we already know (*exploitation*) or (b) result in the discovery of new information (*exploration*). This decision of whether to explore or exploit is directly related to the relative strength of our goals within a particular context. In the cooking example, should I prioritize the goal of learning by cooking the new recipe, or should I emphasize the performance goal by preparing the tried and true meal? The key insight motivating the work reported here is that features of the social context play a fundamental role in the goals we consider and the relative weighting of each goal. And, in turn, the goals that we consider shape the actions take. We present a formal account to integrate social reasoning processes with decision making in the context of learning-performance tradeoff, as a case study of the explore-exploit dilemma.

We situate our integrative account within two theoretical frameworks: *active learning* and *pragmatic social reasoning*. Active learning refers to situations where people are given control over the sequence of information in a learning context (e.g., verbal question asking to elicit informative responses). The key assumption is that learners will maximize the usefulness of their actions by gathering information that is especially helpful for their own learning. The effects of active learning have been the focus of much empirical work in education [@grabinger1995rich; @prince2004does], machine learning [@settles2012active; @ramirez2017active], and cognitive psychology [@castro2009human; @chi2009active], with the common finding that active contexts lead to more rapid learning when compared to passive contexts where people do not have control over the flow of information. 

Work on exploratory actions in active learning often isolates people’s information goals by removing the learner from any kind of social context. In contrast, real-world learning is characterized by contexts where there are teachers, peer learners, or other individuals who can directly influence the utility of information gathering actions. Consider that there is now a large body of evidence suggesting that social reasoning processes can change the computations (i.e., inferences) that support learning from evidence. For example, children learn at different rates after observing the same evidence depending on whether they thought the behavior was accidental (less informative) or intentional (more informative). Moreover, adults and children will make even stronger inferences if they believe that another person selected their actions with the goal of helping them learn (i.e., teaching) [@shafto2012learning]. 

But how can we begin to understand the role of *social* factors in self-directed learning contexts? Answering this question represents a significant step because explore-exploit decisions often unfold within fundamentally social contexts where people must integrate the value of social goals and information goals when deciding what to do next. Consider that actions that maximize learning are inherently risky -- leading to mistakes and struggles -- which could be difficult to undertake in with someone else present. Thus, a learner might prioritize actions that maintain their reputation: looking more attractive, knowledgeable, or competent to the observer. In this case, the learner is leveraging psychological reasoning processes to infer what the observer thinks based on their action (cook a meal) and its outcome (a delicious meal).  If the learner is worried about the observer’s beliefs, then the they might choose actions that maximize the probability of maintaining a favorable impression in the eyes of the observer ('That meal was delicious, so he must be a good cook!'). Indeed, when the value of self-presentation is highlighted, children avoid opportunities that will help them learn new things but pose risks for public mistakes [@elliott1988; @dweck1988]. 

We present the first computational model of the learning-performance tradeoff in a self-directed learning context with social factors. We model a learner who considers his learning goal versus performance goal, which may be influenced by the social context (e.g. the presence of another individual whom he wants to impress). We then look at adult participants’ action choices within a minimalistic self-directed learning task representing different goals and social contexts, and show that people's choices are consistent with predictions of our model.

# Computational model

To start to examine people's exploration-exploitation tradeoff, 
we situate the model and paradigm in a simplistic learning environment.
The learner in our model is to act on a toy,
and can choose between two kinds of actions
that will each lead to one outcome (new discovery) or the other (overt reward).
The learner's action rests on his goals to explore versus exploit, 
and is determined in part by the presence or absence of another person he cares about (i.e. his boss)\footnote{From here on, we use a male pronoun for Bob, the learner, and female pronoun for Ann, the boss and observer.}.

A key assumption underlying inferences in recent Bayesian models of human social cognition is that 
people act approximately optimally given a utility function [e.g. @goodman2016; @jara2016]. 
Our model adopts the same utility-theoretic approach, and assumes an approximately optimal agent, 
who reasons about the utility function that represents a combination of multiple goals. 
In a recent model of polite language production [@yoon2017], 
the utility function comprised a weighted combination of multiple utilities (goals) considered by the speaker, 
reflecting a principled tradeoff between different communicative goals 
(e.g. to be informative, to be kind, and to appear to be a helpful speaker). 
We use a similarly structured utility function that reflects 
different goals that a learner has in a social learning context.
Specifically, we model how a person may make a decision to act based on 
his desire to learn how a toy works (*learning utility*), 
to make the toy operate and perform a given function (*performance utility*), 
or to present himself as a competent individual who knows how to make the toy work (*presentational utility*; 
see the model diagram in Figure 1).

```{r model_diagram, fig.env = "figure*", fig.pos = "h", fig.width=12, fig.height=8, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Diagram of the computational model: The learner considers possible hypotheses and his contextual goals. When an observer is absent, he considers his learning goal (to maximize information gain) and performance goal (e.g. to play music) and decides on an action. When an observer is present, his decision for an action is based on his learning goal vs. presentational goal (to have the observer infer his competence)."}
img1 <-  rasterGrob(as.raster(readPNG("figs/model-obsAbs.png")), interpolate = FALSE)
img2 <-  rasterGrob(as.raster(readPNG("figs/model-obsPres.png")), interpolate = FALSE)
grid.arrange(img1, img2, ncol = 1)
```

First, the *learning utility* symbolizes the goal to learn new information, which in our paradigm specifically is associated with figuring out how a given toy works. The learning utility is formally represented by an OED model ["Optimal Experiment Design"; @nelson2005; @lindley1956], which quantifies the *expected utility* of different information seeking actions. Here we follow the mathematical details of the OED approach as outlined in @coenen2017 that was implemented in our model. The set of queries, each realized through taking an action, is defined as $Q_1, Q_2, ..., Q_n = {Q}$. The expected utility of each query ($EU(Q)$) is a function of two factors: (1) the probability of obtaining a specific answer $P(a)$ weighted by (2) the usefulness of that answer for achieving the learning goal $U(a)$.

$$EU(Q) = \sum_{a\in q}{P(a)U(a)}$$

There are a variety of ways to define the usefulness function to score each answer (for a detailed analysis of different approaches, see @nelson2005). One standard method is to use *information gain*, which is defined as the change in the learner's overall uncertainty (difference in entropy) before and after receiving an answer. This information gain is then the usefulness of the answer to the query, and thus is equal to the learning utility:

$$ U_{learning} = U(a) = ent(H) - ent(H|a)$$
\noindent
where $ent(H)$ is defined using Shannon entropy\footnote{Shannon entropy is a measure of unpredictability or amount of uncertainty in the learner's probability distribution over hypotheses. Intuitively, higher entropy distributions are more uncertain and harder to predict. For example, if the learner believes that all hypotheses are equally likely, then they are in a state of high uncertainty/entropy. In contrast, if the learner firmly believes in one hypothesis, then uncertainty/entropy is low.}. @mackay2003, which provides a measure of the overall amount of uncertainty in the learner's beliefs about the candidate hypotheses. 

$$ent(H) = -\sum_{a\in A}{P(h)log_2P(h)}$$
\noindent
The conditional entropy computation is the same, but takes into account the change in the learner's beliefs after seeing an answer.

$$ ent(H|a) = -\sum_{h\in H}{P(h|a)logP(h|a)} $$
\noindent
To calculate the change in the learner's belief in a hypothesis $P(h|a)$, we use Bayes rule. 

$$ P(h|a) = \frac{P(h)P(a|h)}{P(a)} $$ 

\noindent
The learner performs the expected utility computation for each query in the set of possible queries and picks the one that maximizes utility. In practice, the learner considers each possible answer, scores the answer with the usefulness function, and weights the score using the probability of getting that answer. 
In our paradigm, a learner thinking about the learning utility considers acting on the toy one way over another, and computes how informative a given answer should be in reducing uncertainty about how the toy works.

Second, the *performance utility* is the utility of successfully making the toy operate. Specifically within our current paradigm, the performance utility is the expected utility of music playing ($m$) given the learner's action $a$. 

$$ U_{performance} = P_L(m | a) $$
\noindent
Thus, performance utility is maximized by taking an action that is most likely to make the toy "go" and play music, which is the operation of interest.

When there is no observer present, the learner considers the tradeoff between the learning utility and performance utility, and he determines his action based on a weighted combination of the two utilities: 

$$ U(a;\phi; obs = no) = \phi \cdot U_{learning} + (1-\phi) \cdot U_{performance} ,$$
\noindent
where $\phi$ is a mixture parameter governing the extent to which the learner prioritizes information gain over making the toy play music. 

When there is another person present to observe the learner's action, this observer $O$ reasons about the competence $c$ of the learner $L$ which is equal to whether the learner was able to make the toy work.

$$ P_O(c) \propto P_L(m | a)$$

The learner thinks about how the observer infers the learner's competence, and his *presentational* utility is based on maximizing the apparent competence inferred by the observer.

$$ U_{presentation} = P_O(c) $$
Thus, when there is an observer present, the learner considers the tradeoff between the learning utility and presentational utility: 

$$ U(m;a;\phi; obs = yes) = \phi \cdot U_{learning} + (1-\phi) \cdot U_{presentational}$$
Based on the utility functions above, the learner ($L$) chooses his action $a$ approximately optimally (as per optimality parameter $\lambda$) given his goal weight and observer presence.

$$ P_L(a | \phi, obs) \propto \exp(\lambda \cdot \mathbb{E}[U(a;\phi; obs)])$$

# Experiment 1

In our Experiments, we wanted to look at people's action choices in a minimalistic self-directed learning environment with varying goals and social contexts. 
In Experiment 1, we wanted to confirm that participants will choose  actions based on different goal descriptions, depending on whether we highlight learning goal, performance goal, or presentation goal. 
We were interested to see what people will choose to do when no goal is specified and they are free to explore.
Thus, we asked participants to imagine a scenario in which they were expected to act on a toy with an uncertain causal mechanism, and 
we assigned participants to different goal conditions: 
learning goal (i.e. learn how the toy works); 
performance goal (e.g. make the toy play music);
presentation goal (i.e. impress their boss);
and no goal.

## Method

### Participants

FIXME participants with IP addresses in the United States were recruited on Amazon’s Mechanical Turk. We excluded FIXME partcipants who failed to answer at least two out of three set of manipulation check questions correctly (See Procedure section for details on the manipulation check questions), and thus the remaining FIXME participants were included in our final analysis.

### Stimuli and Design

We presented three different toys that look very similar but each work in different ways, 
and provided instructions for them. 
The "ButtonMusic" toy instructions were: *"Press the button on the right to play music. Pull the handle on the left to turn on the light. Doing both produces both effects."* 
The "HandleMusic" toy instuctions were: *"Pull the handle on the left to play music. Press the button on the right to turn on the light. Doing both produces both effects."* 
The "BothMusicLight" toy instuctions were: *"Pull the handle on the left AND press the button on the right to turn on the light and play music at the same time. The button press or handle pull on its own doesn’t produce any effect."* 
Each toy had a label showing its name.

We presented a story to the participants that motivated a goal the participants must achieve by acting on a given toy. 
Importantly, the given toy was missing its label, such that partcipants could not know whether the toy was a ButtonMusic, HandleMusic, or BothMusicLight toy. 
For all the participants, we asked them to imagine that they were a children's toy developer and that one day, their boss approached them and said: 
"That must be one of the new toys that you've been working on. But it looks like you forgot to put on the label! Can you figure out whether this toy is a ButtonMusic toy, HandleMusic toy, or BothMusicLight toy?" (*learning* condition); 
"That must be one of the new toys that you've been working on. I want to hear the music it plays." (*performance* condition);
or "That must be one of the new toys that you've been working on. How does it work?" followed by the prompt "... [Imagine] you only had one chance to impress your boss and show that you're competent ..." (*presentation* condition). 
We asked participants to select an action they would like to try out on the toy 
in order to accomplish the specified goal, out of three possible actions: 
to "press the button", "pull the handle", or "press the button and pull the handle." 
We randomly assigned each participant to one of the three goal conditions, 
and randomized the order of actions to choose from.

### Procedure

Participants were first introduced to the task with a picture of a toy with labels on its parts. 
Then they read instructions for each of the three toys, 
after which they were asked what they would do to make the toy operate as manipulation check
(e.g. "How would you make the toy play music?").
We asked participants to rate prior likelihood that an unknown toy is a ButtonMusic toy, HandleMusic toy, or BothMusicLight toy, to use as priors for our model. 
Participants then read a scenario for one of the three goal conditions, 
followed by the question: 
"If you only had one chance to try a SINGLE action to [goal], which action would you want to take? 
You will get a 10 cent bonus after submitting the HIT if you [goal]." 
After selecting one of three possible actions to perform on the toy and 
seeing that the toy successfully played music, 
participants were asked again to rate the likelihood that the unlabeled toy was each of the three possible toys. 
Our hypothesis and method were pre-registered prior to data collection on the Open Science Framework (https://osf.io/kcjau).

# Results

## Action selections

## Response times

## Entropy change

# Discussion 

# Acknowledgements

This work was supported by NSERC postgraduate doctoral scholarship PGSD3-454094-2014 to EJY ... FIXME.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
