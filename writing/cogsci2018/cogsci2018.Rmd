---
title: "Should I learn or should I make it go? Balancing informational and social goals in active learning"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Erica J. Yoon*}, {\large \bf Kyle MacDonald*}, {\large \bf Mika Asaba}, {\large \bf Hyowon Gweon}, \and {\large \bf Michael C. Frank} \\ \{ejyoon, kylem4, masaba, hyo, mcfrank\} @stanford.edu \\ Department of Psychology, Stanford University \\ *These authors contributed equally to this work.}

abstract: 
    "The actions we take shape what we learn. Empirical and theoretical work shows that people are capable of efficient self-directed learning to maximize informational goals. But a fundamental feature of human learning is that it unfolds within a social context. How can we begin to understand the role of *social* factors in self-directed learning? Here, we present a computational model that integrates the value of social and information goals to predict the decisions that people will make in a simple active causal learning task. We show that emphasizing performance or self-presentation goals leads to actions that reduce the chances of learning (E1). Next, we show that the presence of an observer (i.e., a boss) pushes learners to emphasize performance/presentation actions even when emphasis is placed on a learning goal (E2). Finally, we present a Bayesian Data Analysis showing that the empirical results are consistent with predictions of our cognitive model of social-active learning. These results represent a first step towards integrating active learning with social reasoning."
    
keywords:
    "active learning; social reasoning; information gain; OED; self-presentation; goal tradeoffs"
    
output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/', echo=F,
                      warning=F, cache=T, message=F, sanitize = T)

image_path <- "writing/cogsci2018/figs/"

library(png); library(grid)
library(gridExtra); library(xtable)
library(tidyverse); library(here)

source(here("R/soc_info_helpers.R"))
```

```{r read data and posterior samples}
d <- read_csv(here("data/03_merged_data/goal_actions_master.csv"))
d_prior <- read_csv(here("data/02_tidy_data/goal_actions_prior_tidy.csv"))
d_models <- readRDS(here("data/03_merged_data/soc-info-posterior-samples.rds"))
d_model_spec <- readRDS(here("data/03_merged_data/soc-info-models.rds"))
```

# Introduction

Imagine that you are a novice cook and you have to decide what meal to prepare for a first date. Should you choose an easy favorite or should you attempt to make something new? While the familiar recipe has a high chance of ensuring a good meal, you are less likely to discover a new, delicious dish. The new recipe might taste even better, but it has a higher chance of failure. But perhaps this decision would change if you were cooking for a friend or teacher who could help or give feedback.

Scenarios like this one capture an "explore-exploit” dilemma [@sutton1998], in which we have to choose between actions that could (a) lead to an overt, readily accessible reward based on what we already know (*exploitation*) or (b) result in the discovery of new information (*exploration*). This decision of whether to explore or exploit is directly related to the relative strength of our goals within a particular context. In the cooking example, should I prioritize the goal of learning by cooking the new recipe, or should I emphasize the performance goal by preparing the tried and true meal? Here, we explore the idea that features of the social context affect the goals we consider, and hence can be translated into this same framework. We present a formal account integrating social reasoning processes with decision making in a simple explore-exploit dilemma.

We situate our integrative account within two theoretical frameworks: *active learning* and *pragmatic social reasoning*. Active learning refers to situations where people are given control over the sequence of information in a learning context (e.g., verbal question asking to elicit informative responses). The key assumption is that learners will maximize the usefulness of their actions by gathering information that is especially helpful for their own learning. The effects of active learning have been the focus of much empirical work in education [@grabinger1995rich], machine learning [@settles2012active], and cognitive psychology [@castro2009human], with the common finding that active contexts lead to more rapid learning when compared to passive contexts where people do not have control over the flow of information. 

```{r model_diagram, fig.env = "figure*", fig.pos = "t", fig.width=4, fig.asp = 0.7, out.width= "100%", fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Diagram of the computational model: The learner considers possible hypotheses and his contextual goals. When an observer is absent, he considers his learning goal (to maximize information gain) and performance goal (e.g. to play music) and decides on an action. When an observer is present, his decision for an action is based on his learning goal vs. presentational goal (to have the observer infer his competence)."}

img1 <- rasterGrob(as.raster(readPNG(here(image_path, "model-obsAbs.png"))), 
                    interpolate = FALSE)

img2 <- rasterGrob(as.raster(readPNG(here(image_path, "model-obsPres.png"))), 
                   interpolate = FALSE)

grid.arrange(img1, img2, ncol = 2)
```

Work on exploratory actions in active learning often isolates information goals by removing the learner from any kind of social context. In contrast, real-world learning is characterized by situations where there are teachers, peer learners, or other individuals who can directly influence the utility of information gathering actions. In fact, a large body of evidence suggests that social reasoning processes shapes how we learn from evidence. For example, children learn faster when observing intentional (more informative) actions compared to accidental (less informative) actions. Moreover, adults and children will make even stronger inferences if they believe that another person selected their actions with the goal of helping them learn (i.e., teaching) [@shafto2012learning]. 

Models of active learning are not typically able to accommodate these richer inferences and richer utility structures where people must integrate the value of social goals -- looking competent or knowledgeable, for example -- and information goals when deciding what to do next. Moreover, actions that maximize learning are inherently risky, and thus are more difficult to undertake in with someone else present. How can active learning models be modified to accommodate this richer set of utilities? As a step towards answering this question, we model a learner who considers a mixture of learning and performance goals. We assume that these goals are triggered by features of the social context such as the presence of another individual whom we want to impress. 

We instantiate the predictions of our model in a simple causal learning task and measure adults’ decisions about whether to take actions that support learning vs. social goals. We show that emphasizing performance or self-presentation goals leads to actions that reduce the chances of learning (E1). Next, we show that the presence of an observer (i.e., a boss) pushes learners to emphasize performance/presentation actions even when emphasis is placed on a learning goal (E2). Finally, we present Bayesian Data Analysis showing that the empirical results are consistent with predictions of our cognitive model of social-active learning.

# Computational model

To examine people's learning-performance tradeoff, 
we situated our model and paradigm in a simple learning environment.
The learner in our model has a toy that he can act on,
and can choose between two kinds of actions
that will each lead to one outcome (new discovery) or the other (immediate reward).
The learner's action rests on his goals to explore versus exploit, 
which in turn are determined in part by the presence or absence of another person he cares about (i.e. his boss)\footnote{From here on, we use a male pronoun for Bob, the learner, and female pronoun for Ann, the boss and observer.}.

A key assumption underlying inferences in recent Bayesian models of human social cognition is that 
people act approximately optimally given a utility function [e.g. @goodman2016; @jara2016]. 
Our model adopts the same utility-theoretic approach, and assumes an approximately optimal agent 
who reasons about the utility function that represents a weighted combination of multiple goals [@yoon2017]. 
Our model thus reflects a principled tradeoff between
different goals that a learner has in a social learning context.
Specifically, we model how a person may make a decision to act based on 
his desire to learn how a toy works (*learning utility*), 
to make the toy operate and perform a given function (*performance utility*), 
or to present himself as a competent individual who knows how to make the toy work (*presentational utility*; see the model diagram in Figure 1).

First, the *learning utility* symbolizes the goal to learn new information, which in our paradigm specifically is associated with figuring out how a given toy works. The learning utility is formally represented by an OED model ["Optimal Experiment Design"; @nelson2005; @lindley1956], which quantifies the *expected utility* of different information seeking actions. Here we follow the mathematical details of the OED approach as outlined in @coenen2017 that was implemented in our model. The set of queries, each realized through taking an action, is defined as $Q_1, Q_2, ..., Q_n = {Q}$. The expected utility of each query ($EU(Q)$) is a function of two factors: (1) the probability of obtaining a specific answer $P(a)$ weighted by (2) the usefulness of that answer for achieving the learning goal $U(a)$.

$$EU(Q) = \sum_{a\in q}{P(a)U(a)}$$

There are a variety of ways to define the usefulness function to score each answer (for a detailed analysis of different approaches, see @nelson2005). One standard method is to use *information gain*, which is defined as the change in the learner's overall uncertainty (difference in entropy) before and after receiving an answer. This information gain is then the usefulness of the answer to the query, and thus is equal to the learning utility ($U_{learn}$):

$$ U_{learn} = U(a) = \frac{ent(H) - ent(H|a)}{log_2n}$$
\noindent
where $ent(H)$ is defined using Shannon entropy\footnote{Shannon entropy is a measure of unpredictability or amount of uncertainty in the learner's probability distribution over hypotheses. Intuitively, higher entropy distributions are more uncertain and harder to predict. For example, if the learner believes that all hypotheses are equally likely, then they are in a state of high uncertainty/entropy. In contrast, if the learner firmly believes in one hypothesis, then uncertainty/entropy is low.}. @mackay2003, which provides a measure of the overall amount of uncertainty in the learner's beliefs about the candidate hypotheses. 

$$ent(H) = -\sum_{a\in A}{P(h)log_2P(h)}$$
\noindent
The conditional entropy computation is the same, but takes into account the change in the learner's beliefs after seeing an answer.

$$ ent(H|a) = -\sum_{h\in H}{P(h|a)logP(h|a)} $$
\noindent
To calculate the change in the learner's belief in a hypothesis $P(h|a)$, we use Bayes rule. 

$$ P(h|a) = \frac{P(h)P(a|h)}{P(a)} $$ 

\noindent
Finally, the difference in entropy is normalized by $log_2 n$.

The learner performs the expected utility computation for each query in the set of possible queries and picks the one that maximizes utility. In practice, the learner considers each possible answer, scores the answer with the usefulness function, and weights the score using the probability of getting that answer. 
In our paradigm, a learner thinking about the learning utility considers acting on the toy one way over another, and computes how informative a given answer should be in reducing uncertainty about how the toy works.

Second, the *performance utility* is the utility of successfully making the toy operate and achieving an immediate rewarding outcome. 
Specifically within our current paradigm, the performance utility ($U_{perf}$) is the expected likelihood of performance (e.g. turning on music; $m$) given the learner's action $a$. 

$$ U_{perf} = P_L(m | a) $$
\noindent
Thus, performance utility is maximized by taking an action that is most likely to make the toy "go" and play music or turn the light on, which is the outcome of interest.

When there is no observer present, the learner considers the tradeoff between the learning utility and performance utility, and he determines his action based on a weighted combination of the two utilities: 

$$ U(a;\phi; obs = no) = \phi_{learn} \cdot U_{learn} + \phi_{perf} \cdot U_{perf} ,$$
\noindent
where $\phi$ is a mixture parameter governing the extent to which the learner prioritizes information gain over immediate reward. 

When there is another person present to observe the learner's action, this observer $O$ is expected to reason about the competence $c$ of the learner $L$ which is equal to whether the learner was able to make the toy produce an effect.

$$ P_O(c) \propto P_L(m | a)$$

The learner thinks about how the observer infers the learner's competence, and his *presentational* utility ($U_{pres}$)is based on maximizing the apparent competence inferred by the observer.

$$ U_{pres} = P_O(c) $$
When there is an observer present, the learner considers the tradeoff between all three utilities: the learning utility, performance utility and presentational utility: 

$$ U(m;a;\phi; obs = yes) = \phi_{learn} \cdot U_{learn} + \\ \phi_{perf} \cdot U_{perf} + \phi_{pres} \cdot U_{pres}$$
Based on the utility functions above, the learner ($L$) chooses his action $a$ approximately optimally (as per optimality parameter $\lambda$) given his goal weight and observer presence.

$$ P_L(a | \phi, obs) \propto \exp(\lambda \cdot \mathbb{E}[U(a;\phi; obs)])$$

# Experiment 1

```{r e1 filter}
d_e1 <- d %>% filter(str_detect(experiment, "e1"))
```

```{r e1_behav_results_plot, include = T, fig.env = "figure*", fig.pos = "tb", fig.width=5, fig.asp = 0.7, out.width= "95%", fig.align='center', fig.cap = "Behavioral results for E1. Panel A shows the proportion of action decisions for each goal condition. Error bars represent 95\\% binomial confidence intervals computed using Bayesian inference. Panel B shows violin plots of participants' response times on the action decisions. Each point represents a participant with the width of the violin representing the density of the data at that value. Panel C shows violin plots of participants' belief change (entropy) as a function of condition. Lower values represent higher certainty after selecting an action. Color in panels A and C represent the type of action participants selected."}
png::readPNG(here::here(image_path, "e1_plot.png")) %>% 
  grid::grid.raster()
```

In Experiment 1, we first wanted to confirm that participants would choose different actions depending on the kind of goal that was highlighted. We were also interested in how people would act when no goal was specified. Importantly, participants were limited to selecting a single action, which meant the opportunity cost for the alternative action was at its highest. Specifically, participants were asked to imagine that they needed to act on a toy with an uncertain causal mechanism, and we assigned them to different goal conditions: (1) learning ("learn how the toy works"), (2) performance ("make the toy play music"), (3) presentation ("impress their boss"), and (4) no goal specified. 

We hypothesized that participants would choose an informative action more often in the following order of goal conditions (decreasing): learning, no goal, performance, and presentation\footnote{Our hypothesis and method were pre-registered prior to data collection on the Open Science Framework (https://osf.io/kcjau)}.

## Method

### Participants

```{r e1 participants}
e1_participants <- d_e1 %>% 
  distinct(id, condition) %>% 
  count(condition)

n_run <- sum(e1_participants$n)

e1_excluded <- d_e1 %>%
  filter(manip_check_score >= 2) %>% 
  distinct(id, condition) %>% 
  count(condition)

n_excluded <- n_run - sum(e1_excluded$n)
n_analyzed <- sum(e1_excluded$n)
```

We recruited `r n_run` participants (roughly 50 for each condition) on Amazon’s Mechanical Turk. To participate participants were required to have an IP addresses in the United States and a task approval rate above 85%. We excluded `r n_excluded` participants who failed to answer at least two out of three manipulation check questions correctly (see Procedure section for details on the manipulation check), and thus the remaining `r n_analyzed` participants were included in our final analysis.

### Stimuli and Design

We presented images of three different toys that look very similar but each work in different ways, and provided instructions for them (see top of Fig. 1 for what the toys looked like).

- Toy 1: *"Pull the handle on the left to turn on the light. Press the button on the right to play music. Doing both produces both effects at the same time."*

- Toy 2: *"Pull the handle on the left to play music. Press the button on the right to turn on the light. Doing both produces both effects at the same time."* 

- Toy 3: *"Pull the handle on the left AND press the button on the right to turn on the light and play music at the same time. The button press or handle pull on its own doesn’t produce any effect."* 

Thus, doing both button press and handle pull was immediately rewarding but uninformative (as it does not disambiguate the causal mechanism in any way), whereas either of the single actions was completely disambiguating, but was uncertain to produce an immediate outcome. Each toy had a label at the front, indicating which action(s) will make the toy operate, and with which outcome effect.

We asked participants to act on one of these toys; importantly, the given toy was missing its label, such that partcipants could not know whether the toy was Toy 1, 2 or 3. We assigned participants into four goal conditions. For participants in *learning*, *performance*, and *presentation* conditions,we asked them to imagine that they were children's toy developers and that one day their boss approached them. We then instructed participants to: figure out the correct label for the toy (*learning* condition); make the toy play music (or turn the light on; *performance* condition); or impress their boss and show that they are competent (*presentation* condition). In *no-goal* condition, participants were asked to select an action. We asked participants to select an action they would like to try out on the toy in order to accomplish the specified goal, out of three possible actions: to "press the button", "pull the handle", or "press the button and pull the handle." We randomly assigned each participant to one of the three goal conditions, and randomized the order of actions to choose from.

### Procedure

We first introduced participants to the task, and showed them a picture of a possible toy with labels on its different parts. Then they read instructions for each of the three toy types. We presented Toy 1 and Toy 2 instructions in a randomized order first, and then Toy 3 instructions. Afterwards, they were asked what they would do to make the toy operate as manipulation check (e.g. "How would you make the toy play music?"). We asked participants to rate prior likelihood that an unknown toy is Toy 1, 2, or 3, to use as priors for our model. Participants then read a scenario for one of the three goal conditions, followed by the question: "If you only had one chance to try a SINGLE action to [pursue the specified goal], which action would you want to take? You will get a 10 cent bonus after submitting the HIT if you [achieve the given goal]." After selecting one of three possible actions to perform on the toy and seeing that the toy successfully played music, participants were asked again to rate the likelihood that the unlabeled toy was each of the three possible toys.

## Results and discussion

### Analysis plan

First, we present behavioral analyses of participants' (1) action decisions, (2) action decision times, and (3) belief change (i.e., learning) ^[See (https://osf.io/kcjau) for a pre-registration of the analysis plan.]. Decision times correspond to the latency to make an action selection as measured from the start of the action decision trial (all RTs were analyzed in log space). We quantified participants' beliefs about the possible toy designs using entropy, and belief change was measured as the difference in entropy before and after selecting an action.

We used the `rstanarm` [@gabry2016rstanarm] package to fit Bayesian regression models estimating the differences across conditions. We report the uncertainty in our point estimates using 95% Highest Density Intervals (HDI). The HDI provides a range of credible values given the data and model. All analysis code for the statistical models can be found in the online repository for this project: https://github.com/kemacdonald/soc-info/R/03_models.Rmd. 

### Action decisions:

```{r actions summarize model contrasts}
e1_contrasts_act <- d_models$action_logit_samples_e1 %>% 
  group_by(effect) %>% 
  summarise(prop = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) 
```

```{r actions get condition betas}
# no-goal is the intercept or reference group
ms_act_e1 <- d_models$action_logit_samples_e1 %>% 
  spread(effect, param_est) %>% 
  mutate(performance = no_goal + performance_beta,
         learning = no_goal + learning_beta,
         presentation = no_goal + presentation_beta) %>% 
  select(sample_id, no_goal, performance, learning, presentation) %>% 
  gather(key = condition, value = param_est, -sample_id) %>% 
  mutate(act_prob_scale = logit_to_prob(param_est)) %>% 
  group_by(condition) %>% 
  summarise(prop = mean(act_prob_scale),
            hdi_lower = quantile(act_prob_scale, probs = 0.025),
            hdi_upper = quantile(act_prob_scale, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) 
```

We modeled action decisions using a logistic regression specified as \texttt{$action \sim goal\_condition$} with the No-Goal condition as the reference category. Participants' tendency to select a "single" action varied across conditions in the predicted pattern (see Panel A of Fig 2), with the highest proportion occuring in the Learning context ($M_{learn} =$ `r ms_act_e1$prop[1] * 100`%, [`r ms_act_e1$hdi_lower[1] * 100`%, `r ms_act_e1$hdi_upper[1] * 100`%]), followed by the No Goal context ($M_{noGoal} =$ `r ms_act_e1$prop[2] * 100`%, [`r ms_act_e1$hdi_lower[2] * 100`%, `r ms_act_e1$hdi_upper[2] * 100`%]), then Performance ($M_{perform} =$ `r ms_act_e1$prop[3] * 100`%, [`r ms_act_e1$hdi_lower[3] * 100`%, `r ms_act_e1$hdi_upper[3] * 100`%]), and the fewest single actions in the Presentation condtion ($M_{present} =$ `r ms_act_e1$prop[4] * 100`%, [`r ms_act_e1$hdi_lower[4] * 100`%, `r ms_act_e1$hdi_upper[4] * 100`%]). 

Compared to the No-Goal condition, participants selected the single action at a greater rate in the Learning condition ($\beta$ = `r e1_contrasts_act$prop[1]`, [`r e1_contrasts_act$hdi_lower[1]`, `r e1_contrasts_act$hdi_upper[1]`]) and at lower rate in the Presentation context ($\beta$ = `r e1_contrasts_act$prop[4]`, [`r e1_contrasts_act$hdi_lower[4]`, `r e1_contrasts_act$hdi_upper[4]`]), with the null value of zero difference condition falling well outside the 95% HDI, and at similar rate in the Performance condition ($\beta$ = `r e1_contrasts_act$prop[3]`, [`r e1_contrasts_act$hdi_lower[3]`, `r e1_contrasts_act$hdi_upper[3]`]) with the 95% HDI including the null.

### Action decision times:

```{r rt contrasts e1}
e1_contrasts_rt <- d_models$rt_samples_e1 %>% 
  group_by(condition) %>% 
  summarise(rt = median(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 1) 
```

```{r rt group means e1}
ms_rt_e1 <- d_models$rt_samples_e1 %>% 
  spread(condition, param_est) %>% 
  mutate(performance = no_goal + performance_beta,
         learning = no_goal + learning_beta,
         presentation = no_goal + presentation_beta) %>% 
  select(sample_id, no_goal, performance, learning, presentation) %>% 
  gather(key = condition, value = rt_sec_scale, -sample_id) %>% 
  group_by(condition) %>% 
  summarise(rt = median(rt_sec_scale),
            hdi_lower = quantile(rt_sec_scale, probs = 0.025),
            hdi_upper = quantile(rt_sec_scale, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 1) 
```

We analyzed response times in log space using the same model specification. Panel A of Figure 2 shows the full RT data distribution. On average, participants took `r ms_rt_e1$rt[2]` seconds to generate a response in the No-goal condidition. Participants took on on average `r e1_contrasts_rt$rt[1]` seconds [`r e1_contrasts_rt$hdi_lower[1]`, `r e1_contrasts_rt$hdi_upper[1]`] longer to generate a decision in the Learning condition, but produced similar response times in the Performance ($M_{perform}$ = `r ms_rt_e1$rt[3]`, [`r ms_rt_e1$hdi_lower[3]`, `r ms_rt_e1$hdi_upper[3]`]) and Presentation ($M_{present}$ = `r ms_rt_e1$rt[4]`, [`r ms_rt_e1$hdi_lower[1]`, `r ms_rt_e1$hdi_upper[4]`]) conditions.

### Belief change:

```{r entropy contrasts e1}
e1_contrasts_ent <- d_models$ent_samples_e1 %>% 
  group_by(effect) %>% 
  summarise(ent_change = median(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) 
```

```{r entropy group means e1}
e1_ent_means <- d_models$ent_samples_e1 %>% 
  select(-eff_type) %>% 
  spread(effect, param_est) %>% 
  mutate(performance_both = no_goal + performance_beta,
         learning_both = no_goal + learning_beta,
         presentation_both = no_goal + presentation_beta,
         no_goal_single = no_goal + single_action_beta,
         performance_single = no_goal + performance_beta + single_action_beta,
         learning_single = no_goal + learning_beta + single_action_beta,
         presentation_single = no_goal + presentation_beta + single_action_beta) %>% 
  rename(no_goal_both = no_goal) %>% 
  select(-contains("beta")) %>% 
  gather(key = condition, value = param_est, -sample_id) %>% 
  group_by(condition) %>% 
  summarise(ent_change = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) 
```

We modeled change in entropy as a function of goal condition and participants' action selections: \texttt{$entropy\_change \sim goal\_condition + action\_response$} (see Panel C of Fig 2) . Across all conditions, people who selected the single action showed a greater reduction in entropy ($\beta$ = `r e1_contrasts_ent$ent_change[5]`, [`r e1_contrasts_ent$hdi_lower[5]`, `r e1_contrasts_ent$hdi_upper[5]`], i.e., learned more from their action. We did not see evidence of an interaction between goal condition and action selection. However, recall that a larger proportion of participants selected the single action in the Learning context, so the probability of learning is higher in this scenario.

# Experiment 2

```{r e2 filter}
d_e2 <- d %>% filter(str_detect(experiment, "e2"))
```

```{r e2_behav_results, include = T, fig.env = "figure*", fig.pos = "tb", fig.width=5, fig.asp = 0.4, out.width= "95%", fig.align='center', fig.cap = "Behavioral results for E2. Panel A shows actions decisions with color representing social context. Panel B shows decision times. Panel C shows belief change. All other plotting conventions are the same as Figure 2."}
png::readPNG(here::here(image_path, "e2_plot.png")) %>% 
  grid::grid.raster()
```

In Experiment 1 we saw that participants made different action choices depending on the goal conditions, as we previously predicted.In Experiment 2, we manipulated goals as well as social contexts, fully crossing the different goal conditions with the presence/absence of the boss, to see whether the social context affects people's decision making differently in each goal condition.

We hypothesized that social pressure should increase presentation-oriented, immediately-rewarding actions in the learning and no-goal conditions, but not in the performance condition in which they are already specified a goal in the same direction.

## Method

### Participants

```{r e2 participants}
e2_participants <- d_e2 %>% 
  distinct(id, condition, social_condition) %>% 
  count(condition, social_condition)

n_run_e2 <- sum(e2_participants$n)

e2_excluded <- d_e2 %>%
  filter(manip_check_score >= 2) %>% 
  distinct(id, condition, social_condition) %>% 
  count(condition, social_condition)

n_excluded_e2 <- n_run_e2 - sum(e2_excluded$n)
n_analyzed_e2 <- sum(e2_excluded$n)
```

We recruited `r n_run_e2` participants (roughly 50 for each condition) on Amazon’s Mechanical Turk. To participate participants were required to have an IP addresses in the United States and a task approval rate above 85%. We excluded `r n_excluded_e2` participants who failed to answer at least two out of three manipulation check questions correctly, and thus the remaining `r n_analyzed_e2` participants were included in our final analysis.

### Stimuli and Design

The stimuli and design were identical to Experiment 1, except we had seven different goal $\times$ social conditions. Goals remained identical to ones presented in Experiment 1; social conditions varied depending on whether the boss was present in the story (*social*) or she was absent (*no-social*). Thus, the conditions from Experiment 1 were used as *social-learning*, *social-performance*, *social-presentation*, and *no-social-no-goal* conditions in Experiment 2. We added three more conditions: *no-social-learning*, *no-social-performance*, and *social-no-goal*. Note that we did not have *no-social-presentation* condition, because presentation goal by definition was to present oneself as competent to and impress another person.  

### Procedure

The procedure was identical to Experiment 1.

## Results and discussion

### Action decisions:

```{r e2 actions summarize model contrasts}
e2_contrasts_act_soc <- d_models$action_logit_samples_e2 %>% 
  group_by(effect) %>% 
  summarise(prop = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 3) 


e2_contrasts_act <- d_models$action_logit_samples_e2_int %>% 
  group_by(effect, eff_type) %>% 
  summarise(prop = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 3) 
```

```{r e2 actions get condition betas}
# no-goal-no-social is the intercept or reference group
ms_act_e2 <- d_models$action_logit_samples_e2 %>% 
  spread(effect, param_est) %>% 
  rename(social_beta = social_conditionsocial) %>% 
  mutate(no_goal_soc = no_goal + social_beta,
         performance_no_soc = no_goal + performance_beta,
         performance_soc = no_goal + performance_beta + social_beta,
         learning_no_soc = no_goal + learning_beta,
         learning_soc = no_goal + learning_beta + social_beta,
         presentation_soc = no_goal + presentation_beta + social_beta) %>% 
  select(- ends_with("_beta")) %>% 
  gather(key = condition, value = param_est, -sample_id) %>% 
  mutate(act_prob_scale = logit_to_prob(param_est)) %>% 
  group_by(condition) %>% 
  summarise(prop = mean(act_prob_scale),
            hdi_lower = quantile(act_prob_scale, probs = 0.025),
            hdi_upper = quantile(act_prob_scale, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) 
```

We modeled action decisions using a logistic regression specified as \texttt{$action \sim goal\_condition * social\_context$} with the No-Goal and No-Social condition as the reference category. We replicated the key finding from E1: participants tended to select the "single" action more often when they were within a context that emphasized a learning goal ($M_{learn} =$ `r ms_act_e2$prop[1] * 100`%, [`r ms_act_e2$hdi_lower[1] * 100`%, `r ms_act_e2$hdi_upper[1] * 100`%]), followed by the No Goal context ($M_{noGoal} =$ `r ms_act_e2$prop[3] * 100`%, [`r ms_act_e2$hdi_lower[3] * 100`%, `r ms_act_e2$hdi_upper[3] * 100`%]), then Performance ($M_{perform} =$ `r ms_act_e2$prop[5] * 100`%, [`r ms_act_e2$hdi_lower[5] * 100`%, `r ms_act_e2$hdi_upper[5] * 100`%]), with the fewest single actions generated in the Presentation condtion ($M_{present} =$ `r ms_act_e2$prop[7] * 100`%, [`r ms_act_e2$hdi_lower[7] * 100`%, `r ms_act_e2$hdi_upper[7] * 100`%]). 

There was a main efffect of social context, with participants being less likely to select the single action when their boss was present ($\beta =$ `r e2_contrasts_act_soc$prop[5] * 100`%, [`r e2_contrasts_act_soc$hdi_lower[5] * 100`%, `r e2_contrasts_act_soc$hdi_upper[5] * 100`%]). Finally, there was evidence for a reliable interaction between goal condition and social context such that the effect of social context was present in the Learning and No-Goal conditions, but not in the Performance condition ($\beta$ $_{int}$ = `r e2_contrasts_act$prop[4]`, [`r e2_contrasts_act$hdi_lower[4] * 100`%, `r e2_contrasts_act$hdi_upper[4] * 100`%]).

### Action decision times:

```{r rt contrasts e2}
e2_contrasts_rt <- d_models$rt_samples_e2 %>% 
  group_by(condition) %>% 
  summarise(rt = median(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) 
```

```{r rt group means e2}
ms_rt_e2 <- d_models$rt_samples_e2 %>% 
  spread(condition, param_est) %>% 
  mutate(performance = no_goal + performance_beta,
         learning = no_goal + learning_beta,
         presentation = no_goal + presentation_beta) %>% 
  select(sample_id, no_goal, performance, learning, presentation) %>% 
  gather(key = condition, value = rt_sec_scale, -sample_id) %>% 
  group_by(condition) %>% 
  summarise(rt = mean(rt_sec_scale),
            hdi_lower = quantile(rt_sec_scale, probs = 0.025),
            hdi_upper = quantile(rt_sec_scale, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 3) 
```

We replicated the key decision time finding from E1: slower decision times in the Learning context. On average, participants took `r ms_rt_e2$m[2]` seconds to generate a response in the No-goal condidition and `r ms_rt_e2$m[1]` seconds in the Learning condition. In contrast, decisions were faster in the Performance ($\beta$ = `r e2_contrasts_rt$rt[3]` sec, [`r e2_contrasts_rt$hdi_lower[3]`, `r e2_contrasts_rt$hdi_upper[3]`]) and Presentation (`r e2_contrasts_rt$rt[4]` seconds, [`r e2_contrasts_rt$hdi_lower[4]`, `r e2_contrasts_rt$hdi_upper[4]`]) conditions, which were similar to one another (see Panel B of Fig 3). There was no evidence of a main effect of social context or an interaction between goal condition and social context. Note that we did not see a difference in decision times between the Learning and No-Goal conditions, which is different from the pattern in E1.

### Belief change:

```{r entropy contrasts e2}
e2_contrasts_ent <- d_models$ent_samples_e2 %>% 
  group_by(effect) %>% 
  summarise(ent_change = mean(param_est),
            hdi_lower = hdi_lower(param_est),
            hdi_upper = hdi_upper(param_est)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) 
```

```{r entropy group means e2}
e2_ent_means <- d_models$ent_samples_e2 %>% 
  spread(effect, param_est) %>% 
  mutate(performance_both = no_goal + performance_beta,
         learning_both = no_goal + learning_beta,
         presentation_both = no_goal + presentation_beta,
         no_goal_single = no_goal + single_action_beta,
         performance_single = no_goal + performance_beta + single_action_beta,
         learning_single = no_goal + learning_beta + single_action_beta,
         presentation_single = no_goal + presentation_beta + single_action_beta) %>% 
  rename(no_goal_both = no_goal) %>% 
  select(-contains("beta")) %>% 
  gather(key = condition, value = param_est, -sample_id) %>% 
  group_by(condition) %>% 
  summarise(ent_change = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) 
```

Across all conditions, participants who selected the single action showed a greater reduction in entropy ($\beta$ = `r e2_contrasts_ent$ent_change[5]`, [`r e2_contrasts_ent$hdi_lower[5]`, `r e2_contrasts_ent$hdi_upper[5]`]. There was some (weaker) evidence of greater reduction in entropy in the Learning goal condition ($\beta$ = `r e2_contrasts_ent$ent_change[1]`, [`r e2_contrasts_ent$hdi_lower[1]`, `r e2_contrasts_ent$hdi_upper[1]`). There was no evidence of a main effect of social context and no two- or three-way interactions between social context, goal condition, and type of action choice.

### BDA model-data fit:

In this experiment, participants were instructed to choose an action based on a certain goal\footnote{A separate prior elicitation task, in which people indicated the likelihood for selecting an action without any background information about possible hypotheses or goals, did not suggest that any of the action choice priors differed from chance; we used mean likelihood for each action choice as the baseline prior in our model; see [FIXME]}. 
We assume that the goal descriptions (e.g. figure out the correct label for the toy; "learning") conveyed to the participants a particular set of goal weights {$\phi_{learn}$, $\phi_{perf}$, $\phi_{pres}$} that they used to generate their action choice. We put uninformative priors on these weights ($\phi \sim Uniform(0,1)$) and inferred their credible values separately for each pair of different goal condition and social context, using Bayesian data analytic techniques [@lee2014bayesian]. 

The inferred goal weights were consistent with what we predicted (see Figure FIXME). 

We also inferred another parameter of the cognitive model, the optimality parameter $\lambda$. We put uninformative prior on the parameter ($\lambda \sim Uniform(0,10)$ and inferred its posterior credible value from the data. We ran 4 MCMC chains for 100,000 iterations, discarding the first 50,000 for burnin. The Maximum A- Posteriori (MAP) estimate and 95% Highest Probability Density Interval (HDI) for $\lambda$ was `r optimality$mean` [`r optimality$ci_lower`, `r optimality$ci_upper`]. 

# General Discussion

# Acknowledgements

This work was supported by NSERC postgraduate doctoral scholarship PGSD3-454094-2014 to EJY and an NSF GRFP to KM [FIXME].

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
