% Template for Cogsci submission with R Markdown

% Stuff changed from original Markdown PLOS Template
\documentclass[10pt, letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{float}
\usepackage{caption}

% amsmath package, useful for mathematical formulas
\usepackage{amsmath}

% amssymb package, useful for mathematical symbols
\usepackage{amssymb}

% hyperref package, useful for hyperlinks
\usepackage{hyperref}

% graphicx package, useful for including eps and pdf graphics
% include graphics with the command \includegraphics
\usepackage{graphicx}

% Sweave(-like)
\usepackage{fancyvrb}
\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{}
\DefineVerbatimEnvironment{Scode}{Verbatim}{fontshape=sl}
\newenvironment{Schunk}{}{}
\DefineVerbatimEnvironment{Code}{Verbatim}{}
\DefineVerbatimEnvironment{CodeInput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{CodeOutput}{Verbatim}{}
\newenvironment{CodeChunk}{}{}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

\usepackage{color}

% Use doublespacing - comment out for single spacing
%\usepackage{setspace}
%\doublespacing


% % Text layout
% \topmargin 0.0cm
% \oddsidemargin 0.5cm
% \evensidemargin 0.5cm
% \textwidth 16cm
% \textheight 21cm

\title{Should I learn or make it go? Balancing informational and social goals
in active learning}


\author{{\large \bf Erica J. Yoon*}, {\large \bf Kyle MacDonald*}, {\large \bf Mika Asaba}, {\large \bf Hyowon Gweon}, \and {\large \bf Michael C. Frank} \\ \{ejyoon, kylem4, masaba, hyo, mcfrank\} @stanford.edu \\ Department of Psychology, Stanford University \\ *These authors contributed equally to this work.}

\begin{document}

\maketitle

\begin{abstract}
The decisions we make shape what we learn. Much empirical and
theoretical work shows that learners will make efficient decisions to
maximize the utility of information. But how can we begin to understand
the role of \emph{social} factors in self-directed learning? Here, we
present a computaional model that integrates the value of social and
information goals to predict the decisions people will make in a simple
active causal learning task.

\textbf{Keywords:}
active learning; social reasoning; information gain; OED;
self-presentation; goal tradeoffs
\end{abstract}

\section{Introduction}\label{introduction}

Imagine that you are a novice cook and you have to decide what meal to
prepare for a first date. Should you choose a recipe that you have tried
before or should you attempt to make something new? While the familiar
recipe has a high chance of ensuring a good meal, you are less likely to
discover a new, delicious dish. The new recipe might taste even better,
but it has a higher chance of turning out awful since you have never
made it before. Now, consider how your decision making might change if
instead of making the meal for first date, you were preparing it for a
charitable cooking teacher whom you trust.

Scenarios like this, capture an ``explore-exploit'' dilemma (Sutton \&
Barto, 1998), in which we have to choose between actions that could (a)
lead to an overt, readily accessible reward based on what we already
know (\emph{exploitation}) or (b) result in the discovery of new
information (\emph{exploration}). This decision of whether to explore or
exploit is directly related to the relative strength of our goals within
a particular context. In the cooking example, should I prioritize the
goal of learning by cooking the new recipe, or should I emphasize the
performance goal by preparing the tried and true meal? The key insight
motivating the work reported here is that features of the social context
play a fundamental role in the goals we consider and the relative
weighting of each goal. And, in turn, the goals that we consider shape
the actions take. We present a formal account to integrate social
reasoning processes with decision making in the context of
learning-performance tradeoff, as a case study of the explore-exploit
dilemma.

We situate our integrative account within two theoretical frameworks:
\emph{active learning} and \emph{pragmatic social reasoning}. Active
learning refers to situations where people are given control over the
sequence of information in a learning context (e.g., verbal question
asking to elicit informative responses). The key assumption is that
learners will maximize the usefulness of their actions by gathering
information that is especially helpful for their own learning. The
effects of active learning have been the focus of much empirical work in
education (Grabinger \& Dunlap, 1995; Prince, 2004), machine learning
(Ramirez-Loaiza, Sharma, Kumar, \& Bilgic, 2017; Settles, 2012), and
cognitive psychology (Castro et al., 2009; Chi, 2009), with the common
finding that active contexts lead to more rapid learning when compared
to passive contexts where people do not have control over the flow of
information.

Work on exploratory actions in active learning often isolates people's
information goals by removing the learner from any kind of social
context. In contrast, real-world learning is characterized by contexts
where there are teachers, peer learners, or other individuals who can
directly influence the utility of information gathering actions.
Consider that there is now a large body of evidence suggesting that
social reasoning processes can change the computations (i.e.,
inferences) that support learning from evidence. For example, children
learn at different rates after observing the same evidence depending on
whether they thought the behavior was accidental (less informative) or
intentional (more informative). Moreover, adults and children will make
even stronger inferences if they believe that another person selected
their actions with the goal of helping them learn (i.e., teaching)
(Shafto, Goodman, \& Frank, 2012).

\begin{CodeChunk}
\begin{figure*}[tb]

{\centering \includegraphics[width=1\linewidth]{figs/model_diagram-1} 

}

\caption[Diagram of the computational model]{Diagram of the computational model: The learner considers possible hypotheses and his contextual goals. When an observer is absent, he considers his learning goal (to maximize information gain) and performance goal (e.g. to play music) and decides on an action. When an observer is present, his decision for an action is based on his learning goal vs. presentational goal (to have the observer infer his competence).}\label{fig:model_diagram}
\end{figure*}
\end{CodeChunk}

But how can we begin to understand the role of \emph{social} factors in
self-directed learning contexts? Answering this question represents a
significant step because explore-exploit decisions often unfold within
fundamentally social contexts where people must integrate the value of
social goals and information goals when deciding what to do next.
Consider that actions that maximize learning are inherently risky --
leading to mistakes and struggles -- which could be difficult to
undertake in with someone else present. Thus, a learner might prioritize
actions that maintain their reputation: looking more attractive,
knowledgeable, or competent to the observer. In this case, the learner
is leveraging psychological reasoning processes to infer what the
observer thinks based on their action (cook a meal) and its outcome (a
delicious meal). If the learner is worried about the observer's beliefs,
then the they might choose actions that maximize the probability of
maintaining a favorable impression in the eyes of the observer (`That
meal was delicious, so he must be a good cook!'). Indeed, when the value
of self-presentation is highlighted, children avoid opportunities that
will help them learn new things but pose risks for public mistakes
(Dweck \& Leggett, 1988; Elliott \& Dweck, 1988).

We present a computational model of the learning-performance tradeoff in
a self-directed learning context with social factors. We model a learner
who considers his learning goal versus performance goal, which may be
influenced by the social context (e.g.~the presence of another
individual whom he wants to impress). We then look at adult
participants' action choices within a minimalistic self-directed
learning task representing different goals and social contexts, and show
that people's choices are consistent with predictions of our model.

\section{Computational model}\label{computational-model}

To examine people's learning-performance tradeoff, we situated our model
and paradigm in a simple learning environment. The learner in our model
has a toy that he can act on, and can choose between two kinds of
actions that will each lead to one outcome (new discovery) or the other
(immediate reward). The learner's action rests on his goals to explore
versus exploit, which in turn are determined in part by the presence or
absence of another person he cares about (i.e.~his
boss)\footnote{From here on, we use a male pronoun for Bob, the learner, and female pronoun for Ann, the boss and observer.}.

A key assumption underlying inferences in recent Bayesian models of
human social cognition is that people act approximately optimally given
a utility function (e.g. Goodman \& Frank, 2016; Jara-Ettinger, Gweon,
Schulz, \& Tenenbaum, 2016). Our model adopts the same utility-theoretic
approach, and assumes an approximately optimal agent who reasons about
the utility function that represents a weighted combination of multiple
goals (Yoon, Tessler, Goodman, \& Frank, 2017). Our model thus reflects
a principled tradeoff between different goals that a learner has in a
social learning context. Specifically, we model how a person may make a
decision to act based on his desire to learn how a toy works
(\emph{learning utility}), to make the toy operate and perform a given
function (\emph{performance utility}), or to present himself as a
competent individual who knows how to make the toy work
(\emph{presentational utility}; see the model diagram in Figure 1).

First, the \emph{learning utility} symbolizes the goal to learn new
information, which in our paradigm specifically is associated with
figuring out how a given toy works. The learning utility is formally
represented by an OED model (Lindley, 1956; ``Optimal Experiment
Design''; Nelson, 2005), which quantifies the \emph{expected utility} of
different information seeking actions. Here we follow the mathematical
details of the OED approach as outlined in Coenen, Nelson, \& Gureckis
(2017) that was implemented in our model. The set of queries, each
realized through taking an action, is defined as
\(Q_1, Q_2, ..., Q_n = {Q}\). The expected utility of each query
(\(EU(Q)\)) is a function of two factors: (1) the probability of
obtaining a specific answer \(P(a)\) weighted by (2) the usefulness of
that answer for achieving the learning goal \(U(a)\).

\[EU(Q) = \sum_{a\in q}{P(a)U(a)}\]

There are a variety of ways to define the usefulness function to score
each answer (for a detailed analysis of different approaches, see Nelson
(2005)). One standard method is to use \emph{information gain}, which is
defined as the change in the learner's overall uncertainty (difference
in entropy) before and after receiving an answer. This information gain
is then the usefulness of the answer to the query, and thus is equal to
the learning utility (\(U_{learn}\)):

\[ U_{learn} = U(a) = \frac{ent(H) - ent(H|a)}{log_2n}\] \noindent
where \(ent(H)\) is defined using Shannon
entropy\footnote{Shannon entropy is a measure of unpredictability or amount of uncertainty in the learner's probability distribution over hypotheses. Intuitively, higher entropy distributions are more uncertain and harder to predict. For example, if the learner believes that all hypotheses are equally likely, then they are in a state of high uncertainty/entropy. In contrast, if the learner firmly believes in one hypothesis, then uncertainty/entropy is low.}.
MacKay (2003), which provides a measure of the overall amount of
uncertainty in the learner's beliefs about the candidate hypotheses.

\[ent(H) = -\sum_{a\in A}{P(h)log_2P(h)}\] \noindent
The conditional entropy computation is the same, but takes into account
the change in the learner's beliefs after seeing an answer.

\[ ent(H|a) = -\sum_{h\in H}{P(h|a)logP(h|a)} \] \noindent
To calculate the change in the learner's belief in a hypothesis
\(P(h|a)\), we use Bayes rule.

\[ P(h|a) = \frac{P(h)P(a|h)}{P(a)} \]

\noindent
Finally, the difference in entropy is normalized by \(log_2 n\).

The learner performs the expected utility computation for each query in
the set of possible queries and picks the one that maximizes utility. In
practice, the learner considers each possible answer, scores the answer
with the usefulness function, and weights the score using the
probability of getting that answer. In our paradigm, a learner thinking
about the learning utility considers acting on the toy one way over
another, and computes how informative a given answer should be in
reducing uncertainty about how the toy works.

Second, the \emph{performance utility} is the utility of successfully
making the toy operate and achieving an immediate rewarding outcome.
Specifically within our current paradigm, the performance utility
(\(U_{perf}\)) is the expected likelihood of performance (e.g.~turning
on music; \(m\)) given the learner's action \(a\).

\[ U_{perf} = P_L(m | a) \] \noindent
Thus, performance utility is maximized by taking an action that is most
likely to make the toy ``go'' and play music or turn the light on, which
is the outcome of interest.

When there is no observer present, the learner considers the tradeoff
between the learning utility and performance utility, and he determines
his action based on a weighted combination of the two utilities:

\[ U(a;\phi; obs = no) = \phi_{learn} \cdot U_{learn} + \phi_{perf} \cdot U_{perf} ,\]
\noindent
where \(\phi\) is a mixture parameter governing the extent to which the
learner prioritizes information gain over immediate reward.

When there is another person present to observe the learner's action,
this observer \(O\) is expected to reason about the competence \(c\) of
the learner \(L\) which is equal to whether the learner was able to make
the toy produce an effect.

\[ P_O(c) \propto P_L(m | a)\]

The learner thinks about how the observer infers the learner's
competence, and his \emph{presentational} utility (\(U_{pres}\))is based
on maximizing the apparent competence inferred by the observer.

\[ U_{pres} = P_O(c) \] When there is an observer present, the learner
considers the tradeoff between all three utilities: the learning
utility, performance utility and presentational utility:

\[ U(m;a;\phi; obs = yes) = \phi_{learn} \cdot U_{learn} + \\ \phi_{perf} \cdot U_{perf} + \phi_{pres} \cdot U_{pres}\]
Based on the utility functions above, the learner (\(L\)) chooses his
action \(a\) approximately optimally (as per optimality parameter
\(\lambda\)) given his goal weight and observer presence.

\[ P_L(a | \phi, obs) \propto \exp(\lambda \cdot \mathbb{E}[U(a;\phi; obs)])\]

\section{Experiment 1}\label{experiment-1}

\begin{CodeChunk}
\begin{figure*}[tb]

{\centering \includegraphics[width=0.95\linewidth]{figs/e1_behav_results_plot-1} 

}

\caption[Behavioral results for E1]{Behavioral results for E1. Panel A shows the proportion of action decisions for each goal condition. Error bars represent 95\% binomial confidence intervals computed using Bayesian inference. Panel B shows violin plots of participants' response times on the action decisions. Each point represents a participant with the width of the violin representing the density of the data at that value. Panel C shows violin plots of participants' belief change (entropy) as a function of condition. Lower values represent higher certainty after selecting an action. Color in panels A and C represent the type of action participants selected.}\label{fig:e1_behav_results_plot}
\end{figure*}
\end{CodeChunk}

In our experimental paradigms, we wanted to look at people's action
choices in a minimalistic self-directed learning environment with
varying goals and social contexts. The task would involve choosing
between an action that is informative but potentially not immediately
rewarding, and an action that is certainly immediately rewarding but not
informative at all.

In Experiment 1, we first wanted to confirm that participants would
choose different actions depending on goals that we highlighted. We were
also interested to see what people would choose to do when no goal is
specified and they are free to explore. Importantly, participants were
limited to making only a single action, which meant the opportunity cost
for the alternative action was at its highest. Thus, participants were
asked to imagine that they needed to act on a toy with an uncertain
causal mechanism, and we assigned participants to different goal
conditions:

\begin{itemize}
\item
  learning goal (i.e.~learn how the toy works)
\item
  performance goal (e.g.~make the toy play music)
\item
  presentation goal (i.e.~impress their boss)
\item
  no goal specified
\end{itemize}

\noindent
We hypothesized that participants would choose an informative action
more often in the following order of goal conditions (decreasing):
learning, no goal, performance, and
presentation\footnote{Our hypothesis and method were pre-registered prior to data collection on the Open Science Framework (https://osf.io/kcjau)}.

\subsection{Method}\label{method}

\subsubsection{Participants}\label{participants}

We recruited 196 participants (roughly 50 for each condition) on
Amazon's Mechanical Turk. To participate participants were required to
have an IP addresses in the United States and a task approval rate above
85\%. We excluded 7 participants who failed to answer at least two out
of three manipulation check questions correctly (see Procedure section
for details on the manipulation check), and thus the remaining 189
participants were included in our final analysis.

\subsubsection{Stimuli and Design}\label{stimuli-and-design}

We presented images of three different toys that look very similar but
each work in different ways, and provided instructions for them (see top
of Fig. 1 for what the toys looked like).

\begin{itemize}
\item
  Toy 1 instructions were: \emph{``Pull the handle on the left to turn
  on the light. Press the button on the right to play music. Doing both
  produces both effects at the same time.''}
\item
  Toy 2 instuctions were: \emph{``Pull the handle on the left to play
  music. Press the button on the right to turn on the light. Doing both
  produces both effects at the same time.''}
\item
  Toy 3 instuctions were: \emph{``Pull the handle on the left AND press
  the button on the right to turn on the light and play music at the
  same time. The button press or handle pull on its own doesn't produce
  any effect.''}
\end{itemize}

Thus, doing both button press and handle pull was immediately rewarding
but uninformative (as it does not disambiguate the causal mechanism in
any way), whereas either of the single actions was completely
disambiguating, but was uncertain to produce an immediate outcome. Each
toy had a label at the front, indicating which action(s) will make the
toy operate, and with which outcome effect.

We asked participants to act on one of these toys; importantly, the
given toy was missing its label, such that partcipants could not know
whether the toy was Toy 1, 2 or 3. We assigned participants into four
goal conditions. For participants in \emph{learning},
\emph{performance}, and \emph{presentation} conditions,we asked them to
imagine that they were children's toy developers and that one day their
boss approached them. We then instructed participants to: figure out the
correct label for the toy (\emph{learning} condition); make the toy play
music (or turn the light on; \emph{performance} condition); or impress
their boss and show that they are competent (\emph{presentation}
condition). In \emph{no-goal} condition, participants were asked to
select an action. We asked participants to select an action they would
like to try out on the toy in order to accomplish the specified goal,
out of three possible actions: to ``press the button'', ``pull the
handle'', or ``press the button and pull the handle.'' We randomly
assigned each participant to one of the three goal conditions, and
randomized the order of actions to choose from.

\subsubsection{Procedure}\label{procedure}

We first introduced participants to the task, and showed them a picture
of a possible toy with labels on its different parts. Then they read
instructions for each of the three toy types. We presented Toy 1 and Toy
2 instructions in a randomized order first, and then Toy 3 instructions.
Afterwards, they were asked what they would do to make the toy operate
as manipulation check (e.g. ``How would you make the toy play music?'').
We asked participants to rate prior likelihood that an unknown toy is
Toy 1, 2, or 3, to use as priors for our model. Participants then read a
scenario for one of the three goal conditions, followed by the question:
``If you only had one chance to try a SINGLE action to {[}pursue the
specified goal{]}, which action would you want to take? You will get a
10 cent bonus after submitting the HIT if you {[}achieve the given
goal{]}.'' After selecting one of three possible actions to perform on
the toy and seeing that the toy successfully played music, participants
were asked again to rate the likelihood that the unlabeled toy was each
of the three possible toys.

\subsection{Results and discussion}\label{results-and-discussion}

\subsubsection{Analysis plan}\label{analysis-plan}

First, we present behavioral analyses of participants' (1) action
decisions, (2) action decision times, and (3) belief change (i.e.,
learning).\footnote{See (\url{https://osf.io/kcjau}) for a
  pre-registration of the analysis plan.} Decision times correspond to
the latency to make an action selection as measured from the start of
the action decision trial (all RTs were analyzed in log space). We
quantified participants' beliefs about the possible toy designs using
entropy, and belief change was measured as the difference in entropy
before and after selecting an action.

We used the \texttt{rstanarm} (Gabry \& Goodrich, 2016) package to fit
Bayesian regression models estimating the differences across conditions.
We report the uncertainty in our point estimates using 95\% Highest
Density Intervals (HDI). The HDI provides a range of credible values
given the data and model. All analysis code for the statistical models
can be found in the online repository for this project:
\url{https://github.com/kemacdonald/soc-info/R/03_models.Rmd}.

Next, we present a Bayesian Data Analysis of the correspondence between
our computational model and participants action responses. {[}FIXME --
say more about the BDA?{]}

\subsubsection{Action decisions:}\label{action-decisions}

We modeled action decisions using a logistic regression specified as
\texttt{$action \sim goal\_condition$} with the No-Goal condition as the
reference category. Participants' tendency to select a ``single'' action
varied across conditions in the predicted pattern (see Panel A of Fig
2), with the highest proportion occuring in the Learning context
(\(M_{learn} =\) 68\%, {[}55\%, 80\%{]}), followed by the No Goal
context (\(M_{noGoal} =\) 38\%, {[}25\%, 52\%{]}), then Performance
(\(M_{perform} =\) 27\%, {[}15\%, 40\%{]}), and the fewest single
actions in the Presentation condtion (\(M_{present} =\) 14\%, {[}6\%,
25\%{]}).

Compared to the No-Goal condition, participants selected the single
action at a greater rate in the Learning condition (\(\beta\) = 1.28,
{[}0.5, 2.17{]}) and at lower rate in the Presentation context
(\(\beta\) = -1.41, {[}-2.47, -0.4{]}), with the null value of zero
difference condition falling well outside the 95\% HDI, and at similar
rate in the Performance condition (\(\beta\) = -0.53, {[}-1.43, 0.35{]})
with the 95\% HDI including the null.

\subsubsection{Action decision times:}\label{action-decision-times}

We analyzed response times in log space using the same model
specification. Panel A of Figure 2 shows the full RT data distribution.
On average, participants took 31 seconds to generate a response in the
No-goal condidition. Participants took on on average 12.2 seconds
{[}4.2, 20{]} longer to generate a decision in the Learning condition,
but produced similar response times in the Performance (\(M_{perform}\)
= 27.5, {[}21.9, 33.3{]}) and Presentation (\(M_{present}\) = 27.5,
{[}37.9, 33.2{]}) conditions.

\subsubsection{Belief change:}\label{belief-change}

We modeled change in entropy as a function of goal condition and
participants' action selections:
\texttt{$entropy\_change \sim goal\_condition + action\_response$} (see
Panel C of Fig 2) . Across all conditions, people who selected the
single action showed a greater reduction in entropy (\(\beta\) = -0.49,
{[}-0.64, -0.33{]}, i.e., learned more from their action. We did not see
evidence of an interaction between goal condition and action selection.
However, recall that a larger proportion of participants selected the
single action in the Learning context, so the probability of learning is
higher in this scenario.

\section{Experiment 2}\label{experiment-2}

\begin{CodeChunk}
\begin{figure*}[tb]

{\centering \includegraphics[width=0.95\linewidth]{figs/e2_behav_results-1} 

}

\caption[Behavioral results for E2]{Behavioral results for E2. Panel A shows actions decisions with color representing social context. Panel B shows decision times. Panel C shows belief change. All other plotting conventions are the same as Figure 2.}\label{fig:e2_behav_results}
\end{figure*}
\end{CodeChunk}

In Experiment 1, we saw that participants made different action choices
depending on the goal conditions, as we previously predicted.In
Experiment 2, we manipulated goals as well as social contexts, fully
crossing the different goal conditions with the presence/absence of the
boss, to see whether the social context affects people's decision making
differently in each goal condition.

We hypothesized that social pressure should increase
presentation-oriented, immediately-rewarding actions in the learning and
no-goal conditions, but not in the performance condition in which they
are already specified a goal in the same direction.

\subsection{Method}\label{method-1}

\subsubsection{Participants}\label{participants-1}

We recruited 347 participants (roughly 50 for each condition) on
Amazon's Mechanical Turk. To participate participants were required to
have an IP addresses in the United States and a task approval rate above
85\%. We excluded 22 participants who failed to answer at least two out
of three manipulation check questions correctly, and thus the remaining
325 participants were included in our final analysis.

\subsubsection{Stimuli and Design}\label{stimuli-and-design-1}

The stimuli and design were identical to Experiment 1, except we had
seven different goal \(\times\) social conditions. Goals remained
identical to ones presented in Experiment 1; social conditions varied
depending on whether the boss was present in the story (\emph{social})
or she was absent (\emph{no-social}). Thus, the conditions from
Experiment 1 were used as \emph{social-learning},
\emph{social-performance}, \emph{social-presentation}, and
\emph{no-social-no-goal} conditions in Experiment 2. We added three more
conditions: \emph{no-social-learning}, \emph{no-social-performance}, and
\emph{social-no-goal}. Note that we did not have
\emph{no-social-presentation} condition, because presentation goal by
definition was to present oneself as competent to and impress another
person.

\subsubsection{Procedure}\label{procedure-1}

The procedure was identical to Experiment 1.

\subsection{Results and discussion}\label{results-and-discussion-1}

\subsubsection{Action decisions:}\label{action-decisions-1}

We modeled action decisions using a logistic regression specified as
\texttt{$action \sim goal\_condition * social\_context$} with the
No-Goal and No-Social condition as the reference category. We replicated
the key finding from E1: participants tended to select the ``single''
action more often when they were within a context that emphasized a
learning goal (\(M_{learn} =\) 68\%, {[}57\%, 79\%{]}), followed by the
No Goal context (\(M_{noGoal} =\) 52\%, {[}40\%, 63\%{]}), then
Performance (\(M_{perform} =\) 37\%, {[}26\%, 48\%{]}), with the fewest
single actions generated in the Presentation condtion (\(M_{present} =\)
8\%, {[}2\%, 17\%{]}).

There was a main efffect of social context, with participants being less
likely to select the single action when their boss was present
(\(\beta =\) -52.2\%, {[}-99.9\%, -4.8\%{]}). Finally, there was
evidence for a reliable interaction between goal condition and social
context such that the effect of social context was present in the
Learning and No-Goal conditions, but not in the Performance condition
(\(\beta\) \(_{int}\) = 1.127, {[}0.9\%, 225.3\%{]}).

\subsubsection{Action decision times:}\label{action-decision-times-1}

We replicated the key decision time finding from E1: slower decision
times in the Learning context. On average, participants took seconds to
generate a response in the No-goal condidition and seconds in the
Learning condition. In contrast, decisions were faster in the
Performance (\(\beta\) = -7.78 sec, {[}-14.01, -1.52{]}) and
Presentation (-10.77 seconds, {[}-18.67, -2.73{]}) conditions, which
were similar to one another (see Panel B of Fig 3). There was no
evidence of a main effect of social context or an interaction between
goal condition and social context. Note that we did not see a difference
in decision times between the Learning and No-Goal conditions, which is
different from the pattern in E1.

\subsubsection{Belief change:}\label{belief-change-1}

Across all conditions, participants who selected the single action
showed a greater reduction in entropy (\(\beta\) = -0.35, {[}-0.45,
-0.24{]}. There was some (weaker) evidence of greater reduction in
entropy in the Learning goal condition (\(\beta\) = -0.12, {[}-0.25,
0.01). There was no evidence of a main effect of social context and no
two- or three-way interactions between social context, goal condition,
and type of action choice.

\subsubsection{BDA model-data fit:}\label{bda-model-data-fit}

In this experiment, participants were instructed to choose an action
based on a certain goal. We assume that the goal descriptions
(e.g.~figure out the correct label for the toy; ``learning'') conveyed
to the participants a particular set of goal weights \{\(\phi_{learn}\),
\(\phi_{perf}\), \(\phi_{pres}\)\} that they used to generate their
action choice. We put uninformative priors on these weights
(\(\phi \sim Uniform(0,1)\)) and inferred their credible values
separately for each pair of different goal condition and social context,
using Bayesian data analytic techniques (Lee \& Wagenmakers, 2014).

The inferred goal weights were consistent with what we predicted
\ldots{} {[}FIXME{]}

We also inferred another parameter of the cognitive model, the
optimality parameter \(\lambda\). We put uninformative prior on the
parameter (\(\lambda \sim Uniform(0,10)\) and inferred its posterior
credible value from the data. We ran 4 MCMC chains for 100,000
iterations, discarding the first 50,000 for burnin. The Maximum A-
Posteriori (MAP) estimate and 95\% Highest Probability Density Interval
(HDI) for \(\lambda\) was 5.76 {[}3.84, 7.14{]}.

\section{General Discussion}\label{general-discussion}

\section{Acknowledgements}\label{acknowledgements}

This work was supported by NSERC postgraduate doctoral scholarship
PGSD3-454094-2014 to EJY and an NSF GRFP to KM {[}FIXME{]}.

\section{References}\label{references}

\setlength{\parindent}{-0.1in} \setlength{\leftskip}{0.125in} \noindent

\hypertarget{refs}{}
\hypertarget{ref-castro2009human}{}
Castro, R. M., Kalish, C., Nowak, R., Qian, R., Rogers, T., \& Zhu, X.
(2009). Human active learning. In \emph{Advances in neural information
processing systems} (pp. 241--248).

\hypertarget{ref-chi2009active}{}
Chi, M. T. (2009). Active-constructive-interactive: A conceptual
framework for differentiating learning activities. \emph{Topics in
Cognitive Science}, \emph{1}(1), 73--105.

\hypertarget{ref-coenen2017}{}
Coenen, A., Nelson, J. D., \& Gureckis, T. M. (2017). Asking the right
questions about human inquiry.

\hypertarget{ref-dweck1988}{}
Dweck, C. S., \& Leggett, E. L. (1988). A social-cognitive approach to
motivation and personality. \emph{Psychological Review}, \emph{95}(2),
256.

\hypertarget{ref-elliott1988}{}
Elliott, E. S., \& Dweck, C. S. (1988). Goals: An approach to motivation
and achievement. \emph{Journal of Personality and Social Psychology},
\emph{54}(1), 5.

\hypertarget{ref-gabry2016rstanarm}{}
Gabry, J., \& Goodrich, B. (2016). Rstanarm: Bayesian applied regression
modeling via stan. r package version 2.10. 0.

\hypertarget{ref-goodman2016}{}
Goodman, N. D., \& Frank, M. C. (2016). Pragmatic language
interpretation as probabilistic inference. \emph{Trends in Cognitive
Sciences}, \emph{20}(11), 818--829.

\hypertarget{ref-grabinger1995rich}{}
Grabinger, R. S., \& Dunlap, J. C. (1995). Rich environments for active
learning: A definition. \emph{ALT-J}, \emph{3}(2), 5--34.

\hypertarget{ref-jara2016}{}
Jara-Ettinger, J., Gweon, H., Schulz, L. E., \& Tenenbaum, J. B. (2016).
The na√Øve utility calculus: Computational principles underlying
commonsense psychology. \emph{Trends in Cognitive Sciences},
\emph{20}(8), 589--604.

\hypertarget{ref-lee2014bayesian}{}
Lee, M. D., \& Wagenmakers, E.-J. (2014). \emph{Bayesian cognitive
modeling: A practical course}. Cambridge university press.

\hypertarget{ref-lindley1956}{}
Lindley, D. V. (1956). On a measure of the information provided by an
experiment. \emph{The Annals of Mathematical Statistics}, 986--1005.

\hypertarget{ref-mackay2003}{}
MacKay, D. J. (2003). \emph{Information theory, inference and learning
algorithms}. Cambridge university press.

\hypertarget{ref-nelson2005}{}
Nelson, J. D. (2005). Finding useful questions: On bayesian
diagnosticity, probability, impact, and information gain.
\emph{Psychological Review}, \emph{112}(4).

\hypertarget{ref-prince2004does}{}
Prince, M. (2004). Does active learning work? A review of the research.
\emph{Journal of Engineering Education}, \emph{93}(3), 223--231.

\hypertarget{ref-ramirez2017active}{}
Ramirez-Loaiza, M. E., Sharma, M., Kumar, G., \& Bilgic, M. (2017).
Active learning: An empirical study of common baselines. \emph{Data
Mining and Knowledge Discovery}, \emph{31}(2), 287--313.

\hypertarget{ref-settles2012active}{}
Settles, B. (2012). Active learning. \emph{Synthesis Lectures on
Artificial Intelligence and Machine Learning}, \emph{6}(1), 1--114.

\hypertarget{ref-shafto2012learning}{}
Shafto, P., Goodman, N. D., \& Frank, M. C. (2012). Learning from
others: The consequences of psychological reasoning for human learning.
\emph{Perspectives on Psychological Science}, \emph{7}(4), 341--351.

\hypertarget{ref-sutton1998}{}
Sutton, R. S., \& Barto, A. G. (1998). \emph{Introduction to
reinforcement learning} (Vol. 135). MIT Press Cambridge.

\hypertarget{ref-yoon2017}{}
Yoon, E. J., Tessler, M. H., Goodman, N. D., \& Frank, M. C. (2017). ``I
won't lie, it wasn't amazing'': Modeling polite indirect speech. In
\emph{Proceedings of the thirty-ninth annual conference of the Cognitive
Science Society}.

\end{document}
